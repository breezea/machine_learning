{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "text_paths = list(map( lambda x: './data/honglou/'+x,os.listdir('./data/honglou/')))\n",
    "\n",
    "# text_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class File(object):\n",
    "    def __init__(self, ):\n",
    "        pass\n",
    "\n",
    "    def write(self, path,  body, encoding='utf8'):\n",
    "        with open(path, 'w', encoding=encoding) as f:\n",
    "            f.write(body)\n",
    "    def append(self, path, body, encoding='utf8'):\n",
    "        with open(path, 'a', encoding=encoding) as f:\n",
    "            f.write(body)\n",
    "    def read_to_str(self, path, encoding='utf8'):\n",
    "        with open(path, 'r', encoding=encoding) as f:\n",
    "            return ''.join(f.readlines()).replace('\\n','')\n",
    "    def file_map(self, path, fn, encoding='utf8'):\n",
    "        with open(path, 'r', encoding='utf8') as f:\n",
    "            for sentence in f.readlines():\n",
    "                fn(sentence, path)\n",
    "                \n",
    "file = File()\n",
    "# file.read_to_str('./data/honglou/第100章.txt')\n",
    "# cut red_water into documents\n",
    "\n",
    "# chapter_mark = '------------\\n'\n",
    "# chapter_count = 0\n",
    "# with open('./data/red_water.txt', encoding='utf8') as fp:\n",
    "#     chapter_body = ''\n",
    "#     for sentence in fp.readlines():\n",
    "#         if sentence == chapter_mark:\n",
    "#             # skip the first time\n",
    "#             if chapter_count == 0: \n",
    "#                 chapter_count += 1\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 # write the text\n",
    "#                 chapter_title = f'第{chapter_count}章.txt'\n",
    "#                 file.write('./data/honglou/'+ chapter_title, chapter_body)\n",
    "#                 # clean variables\n",
    "#                 chapter_count += 1\n",
    "#                 chapter_body = '' \n",
    "#         else: \n",
    "#             chapter_body += sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import numpy as np\n",
    "class IDF(File):\n",
    "    def __init__(self, text_paths):\n",
    "        super(File).__init__()\n",
    "        self.idf_store = {}\n",
    "        self.idf_store_pre = {}\n",
    "        self.N = None\n",
    "        self.train(text_paths)\n",
    "    def train(self, text_paths):\n",
    "        '''\n",
    "        params:\n",
    "            text_paths: all doucment paths\n",
    "        return: \n",
    "            idf dictory (key: qi, value: score of idf)\n",
    "        '''\n",
    "        self.N  = len(text_paths) #number of document \n",
    "        def process_line(sentence, path):\n",
    "            sentence = self.__clean(sentence)\n",
    "            words = jieba.cut_for_search(sentence)\n",
    "            for word in words:\n",
    "                self.__set_idf_store_pre(word, path)\n",
    "        for text_path in text_paths:\n",
    "            self.file_map(text_path, process_line)\n",
    "        for word,docs in self.idf_store_pre.items():\n",
    "            self.idf_store[word] = len(docs) \n",
    "\n",
    "    def __clean(self,string):\n",
    "        string = string.strip()\n",
    "        return string\n",
    "    def __set_idf_store_pre(self, word, text_path):\n",
    "        if self.idf_store_pre.get(word, False):\n",
    "            if text_path not in self.idf_store_pre[word]:\n",
    "                self.idf_store_pre[word].append(text_path)\n",
    "        else:\n",
    "            self.idf_store_pre[word] = [text_path]\n",
    "    def get(self, word):\n",
    "        '''\n",
    "        params:\n",
    "            word : that you to know its idf\n",
    "        return:\n",
    "            word's idf\n",
    "        '''\n",
    "        n_qi = self.idf_store.get(word, self.N)\n",
    "        return np.log( (self.N - n_qi + 0.5) / (n_qi + 0.5) )\n",
    "\n",
    "# text_paths = ['./data/honglou/第100章.txt',  './data/honglou/第101章.txt']\n",
    "\n",
    "# idf = IDF(text_paths)\n",
    "# idf.train(text_paths)\n",
    "# idf.idf_store\n",
    "\n",
    "# idf.get('贾雨村')\n",
    "# idf.get('贾雨')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\lenovo\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.434 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "class BM25(object):\n",
    "\n",
    "    def __init__(self, text_paths):\n",
    "        self.text_paths = text_paths\n",
    "        self.fre_dict = {}  # word'd appear frequence\n",
    "        self.dl =  {} # lenght of document\n",
    "        #hyporparameters\n",
    "        self.k1 = 2\n",
    "        self.b = 0.75\n",
    "\n",
    "    def train(self, ):\n",
    "        self.idf = IDF(text_paths)\n",
    "        # record document length (dl)\n",
    "        for text_path in self.text_paths:\n",
    "            file.file_map(text_path, self.__process_sentence)\n",
    "            self.dl[text_path] = len( file.read_to_str(text_path) )\n",
    "        # documenet average length\n",
    "        self.dl['avgdl'] = sum(self.dl.values()) / len(self.text_paths)\n",
    "    \n",
    "    def __process_sentence(self, sentence, path):\n",
    "        def set_fre_dict(key, doc_id):\n",
    "            if self.fre_dict.get(key, False):\n",
    "                if self.fre_dict[key].get(doc_id, False):\n",
    "                    self.fre_dict[key][doc_id] += 1\n",
    "                else:\n",
    "                    self.fre_dict[key][doc_id] = 1\n",
    "            else:\n",
    "                self.fre_dict[key] = {doc_id: 1}\n",
    "        sentence = sentence.strip()\n",
    "        words = jieba.cut(sentence)\n",
    "        for word in words:\n",
    "            set_fre_dict(word, path)\n",
    "\n",
    "    def query(self, sentence, n=8):\n",
    "        '''\n",
    "        params:\n",
    "            sentence: the input sentence\n",
    "            R: score for word and document\n",
    "        return:\n",
    "            score for current text\n",
    "        '''\n",
    "        word_scores = []\n",
    "        words = list(jieba.cut(sentence))\n",
    "        for text_path in self.text_paths:\n",
    "            record = []\n",
    "            for word in words:\n",
    "                # wi: weight of current word, acrually idf\n",
    "                w = self.idf.get(word)\n",
    "                r = self.R(word, text_path)\n",
    "                record.append(w*r)\n",
    "            word_scores.append({\n",
    "                'text_paths': text_path,\n",
    "                'score': sum(record)\n",
    "            })\n",
    "        word_scores.sort(key=lambda x: x['score'], reverse=True)\n",
    "        return word_scores[:n]\n",
    "\n",
    "    def R(self, qi,doc_id):\n",
    "        '''\n",
    "        params:\n",
    "            qi: the word factor \n",
    "            d: documnet\n",
    "        return: \n",
    "            score for word(qi) and document(d) \n",
    "        '''\n",
    "        dl =  self.dl[doc_id]\n",
    "        avgdl = self.dl['avgdl']\n",
    "        try:\n",
    "            f_i = self.fre_dict[qi][doc_id]\n",
    "        except Exception as e:\n",
    "            # print(e)\n",
    "            f_i = 0\n",
    "        K = self.k1 * ( 1-self.b + self.b * (dl/ avgdl) )\n",
    "        R = (f_i * self.k1) / (f_i + K)\n",
    "        # print(dl, avgdl, f_i, K, R)\n",
    "        return R\n",
    "\n",
    "# text_paths = ['./data/honglou/第100章.txt',  './data/honglou/第101章.txt', './data/honglou/第101章.txt']\n",
    "bm25 = BM25(text_paths)\n",
    "bm25.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text_paths': './data/honglou/第27章.txt', 'score': 2.5858814153760137},\n",
       " {'text_paths': './data/honglou/第30章.txt', 'score': 2.5038802146392842},\n",
       " {'text_paths': './data/honglou/第72章.txt', 'score': 1.9514468391839017},\n",
       " {'text_paths': './data/honglou/第31章.txt', 'score': 1.0251785095897015},\n",
       " {'text_paths': './data/honglou/第92章.txt', 'score': 1.0087800990678442},\n",
       " {'text_paths': './data/honglou/第28章.txt', 'score': 0.8444882659010499},\n",
       " {'text_paths': './data/honglou/第101章.txt', 'score': 0.0},\n",
       " {'text_paths': './data/honglou/第102章.txt', 'score': 0.0}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bm25.query('刘姥姥大观园')\n",
    "# bm25.query('姥姥观园')\n",
    "bm25.query('黛玉葬花',)\n",
    "# bm25.query('贾宝玉')\n",
    "# list(jieba.cut('林黛玉'))\n",
    "# bm25.R('宝玉',text_paths[0])\n",
    "# bm25.idf.get('宝玉')\n",
    "# bm25.query('宝玉')\n",
    "# text_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0ff7822346a7eba5f653fb958b3cf569954cc4bb3d216a01aedeb8a9d21f674b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
